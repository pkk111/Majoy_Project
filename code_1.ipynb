{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_shape: [72, 72, 3]\n",
      "image_data: 26161\n",
      "labels: 26161\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "from keras.preprocessing.image import img_to_array\n",
    "\n",
    "shared_dir = '/Users/prateek/Desktop/ML/Major/'\n",
    "infected_dir = shared_dir + 'True_parasitized/'\n",
    "uninfected_dir = shared_dir + 'True_uninfected/'\n",
    "input_shape = [72, 72]\n",
    "batch = 32\n",
    "\n",
    "parasitized_data = os.listdir(infected_dir)\n",
    "uninfected_data = os.listdir(uninfected_dir)\n",
    "\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "for img in parasitized_data:\n",
    "    try:\n",
    "        img_read = plt.imread(infected_dir + img)\n",
    "        img_resize = cv2.resize(img_read, input_shape)\n",
    "        img_array = img_to_array(img_resize)\n",
    "        data.append(img_array)\n",
    "        labels.append(1)\n",
    "    except :\n",
    "        None\n",
    "        \n",
    "for img in uninfected_data:\n",
    "    try:\n",
    "        img_read = plt.imread(uninfected_dir + img)\n",
    "        img_resize = cv2.resize(img_read, input_shape)\n",
    "        img_array = img_to_array(img_resize)\n",
    "        data.append(img_array)\n",
    "        labels.append(0)\n",
    "    except:\n",
    "        None\n",
    "\n",
    "input_shape.append(3)\n",
    "image_data = np.array(data)\n",
    "labels = np.array(labels)\n",
    "\n",
    "print(\"image_shape:\", input_shape)\n",
    "print(\"image_data:\",len(image_data))\n",
    "print(\"labels:\",len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n",
      "\n",
      "systemMemory: 16.00 GB\n",
      "maxCacheSize: 5.33 GB\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-24 11:43:14.222826: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-04-24 11:43:14.223165: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "\n",
    "data_augmentation = keras.Sequential(\n",
    "    [       tf.keras.layers.experimental.preprocessing.RandomFlip(\"horizontal\"),\n",
    "            tf.keras.layers.experimental.preprocessing.RandomRotation(0.1),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "When setting `include_top=True` and loading `imagenet` weights, `input_shape` should be (299, 299, 3).  Received: input_shape=[72, 72, 3]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/prateek/Desktop/ML/Major/code_1.ipynb Cell 3'\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/prateek/Desktop/ML/Major/code_1.ipynb#ch0000002?line=4'>5</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapplications\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mxception\u001b[39;00m \u001b[39mimport\u001b[39;00m Xception, preprocess_input\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/prateek/Desktop/ML/Major/code_1.ipynb#ch0000002?line=6'>7</a>\u001b[0m epochs \u001b[39m=\u001b[39m \u001b[39m200\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/prateek/Desktop/ML/Major/code_1.ipynb#ch0000002?line=8'>9</a>\u001b[0m base_model \u001b[39m=\u001b[39m Xception(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/prateek/Desktop/ML/Major/code_1.ipynb#ch0000002?line=9'>10</a>\u001b[0m   weights\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mimagenet\u001b[39;49m\u001b[39m'\u001b[39;49m,  \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/prateek/Desktop/ML/Major/code_1.ipynb#ch0000002?line=10'>11</a>\u001b[0m   input_shape\u001b[39m=\u001b[39;49minput_shape,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/prateek/Desktop/ML/Major/code_1.ipynb#ch0000002?line=11'>12</a>\u001b[0m   include_top\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/prateek/Desktop/ML/Major/code_1.ipynb#ch0000002?line=12'>13</a>\u001b[0m base_model\u001b[39m.\u001b[39mtrainable \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/prateek/Desktop/ML/Major/code_1.ipynb#ch0000002?line=15'>16</a>\u001b[0m inputs \u001b[39m=\u001b[39m Input(shape\u001b[39m=\u001b[39minput_shape)\n",
      "File \u001b[0;32m~/venv/ml/lib/python3.8/site-packages/keras/applications/xception.py:127\u001b[0m, in \u001b[0;36mXception\u001b[0;34m(include_top, weights, input_tensor, input_shape, pooling, classes, classifier_activation)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/prateek/venv/ml/lib/python3.8/site-packages/keras/applications/xception.py?line=122'>123</a>\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mIf using `weights` as `\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mimagenet\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m` with `include_top`\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    <a href='file:///Users/prateek/venv/ml/lib/python3.8/site-packages/keras/applications/xception.py?line=123'>124</a>\u001b[0m                    \u001b[39m'\u001b[39m\u001b[39m as true, `classes` should be 1000\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    <a href='file:///Users/prateek/venv/ml/lib/python3.8/site-packages/keras/applications/xception.py?line=125'>126</a>\u001b[0m \u001b[39m# Determine proper input shape\u001b[39;00m\n\u001b[0;32m--> <a href='file:///Users/prateek/venv/ml/lib/python3.8/site-packages/keras/applications/xception.py?line=126'>127</a>\u001b[0m input_shape \u001b[39m=\u001b[39m imagenet_utils\u001b[39m.\u001b[39;49mobtain_input_shape(\n\u001b[1;32m    <a href='file:///Users/prateek/venv/ml/lib/python3.8/site-packages/keras/applications/xception.py?line=127'>128</a>\u001b[0m     input_shape,\n\u001b[1;32m    <a href='file:///Users/prateek/venv/ml/lib/python3.8/site-packages/keras/applications/xception.py?line=128'>129</a>\u001b[0m     default_size\u001b[39m=\u001b[39;49m\u001b[39m299\u001b[39;49m,\n\u001b[1;32m    <a href='file:///Users/prateek/venv/ml/lib/python3.8/site-packages/keras/applications/xception.py?line=129'>130</a>\u001b[0m     min_size\u001b[39m=\u001b[39;49m\u001b[39m71\u001b[39;49m,\n\u001b[1;32m    <a href='file:///Users/prateek/venv/ml/lib/python3.8/site-packages/keras/applications/xception.py?line=130'>131</a>\u001b[0m     data_format\u001b[39m=\u001b[39;49mbackend\u001b[39m.\u001b[39;49mimage_data_format(),\n\u001b[1;32m    <a href='file:///Users/prateek/venv/ml/lib/python3.8/site-packages/keras/applications/xception.py?line=131'>132</a>\u001b[0m     require_flatten\u001b[39m=\u001b[39;49minclude_top,\n\u001b[1;32m    <a href='file:///Users/prateek/venv/ml/lib/python3.8/site-packages/keras/applications/xception.py?line=132'>133</a>\u001b[0m     weights\u001b[39m=\u001b[39;49mweights)\n\u001b[1;32m    <a href='file:///Users/prateek/venv/ml/lib/python3.8/site-packages/keras/applications/xception.py?line=134'>135</a>\u001b[0m \u001b[39mif\u001b[39;00m input_tensor \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///Users/prateek/venv/ml/lib/python3.8/site-packages/keras/applications/xception.py?line=135'>136</a>\u001b[0m   img_input \u001b[39m=\u001b[39m layers\u001b[39m.\u001b[39mInput(shape\u001b[39m=\u001b[39minput_shape)\n",
      "File \u001b[0;32m~/venv/ml/lib/python3.8/site-packages/keras/applications/imagenet_utils.py:349\u001b[0m, in \u001b[0;36mobtain_input_shape\u001b[0;34m(input_shape, default_size, min_size, data_format, require_flatten, weights)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/prateek/venv/ml/lib/python3.8/site-packages/keras/applications/imagenet_utils.py?line=346'>347</a>\u001b[0m   \u001b[39mif\u001b[39;00m input_shape \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///Users/prateek/venv/ml/lib/python3.8/site-packages/keras/applications/imagenet_utils.py?line=347'>348</a>\u001b[0m     \u001b[39mif\u001b[39;00m input_shape \u001b[39m!=\u001b[39m default_shape:\n\u001b[0;32m--> <a href='file:///Users/prateek/venv/ml/lib/python3.8/site-packages/keras/applications/imagenet_utils.py?line=348'>349</a>\u001b[0m       \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mWhen setting `include_top=True` \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    <a href='file:///Users/prateek/venv/ml/lib/python3.8/site-packages/keras/applications/imagenet_utils.py?line=349'>350</a>\u001b[0m                        \u001b[39m'\u001b[39m\u001b[39mand loading `imagenet` weights, \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    <a href='file:///Users/prateek/venv/ml/lib/python3.8/site-packages/keras/applications/imagenet_utils.py?line=350'>351</a>\u001b[0m                        \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m`input_shape` should be \u001b[39m\u001b[39m{\u001b[39;00mdefault_shape\u001b[39m}\u001b[39;00m\u001b[39m.  \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    <a href='file:///Users/prateek/venv/ml/lib/python3.8/site-packages/keras/applications/imagenet_utils.py?line=351'>352</a>\u001b[0m                        \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mReceived: input_shape=\u001b[39m\u001b[39m{\u001b[39;00minput_shape\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m    <a href='file:///Users/prateek/venv/ml/lib/python3.8/site-packages/keras/applications/imagenet_utils.py?line=352'>353</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m default_shape\n\u001b[1;32m    <a href='file:///Users/prateek/venv/ml/lib/python3.8/site-packages/keras/applications/imagenet_utils.py?line=353'>354</a>\u001b[0m \u001b[39mif\u001b[39;00m input_shape:\n",
      "\u001b[0;31mValueError\u001b[0m: When setting `include_top=True` and loading `imagenet` weights, `input_shape` should be (299, 299, 3).  Received: input_shape=[72, 72, 3]"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Dense, Dropout, Flatten, GlobalAveragePooling2D\n",
    "from keras import Model, Input\n",
    "from keras.applications.xception import Xception, preprocess_input\n",
    "\n",
    "epochs = 200\n",
    "\n",
    "base_model = Xception(\n",
    "  weights='imagenet',  \n",
    "  input_shape=input_shape,\n",
    "  include_top=False)\n",
    "base_model.trainable = False\n",
    "\n",
    "\n",
    "inputs = Input(shape=input_shape)\n",
    "x = data_augmentation(inputs)\n",
    "x = preprocess_input(inputs)\n",
    "x = base_model(x, training=False)\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "x = Flatten()(x)\n",
    "outputs = Dense(1)(x)\n",
    "model = Model(inputs, outputs)\n",
    "print(model.summary())\n",
    "\n",
    "model.compile(optimizer='sgd', loss=keras.losses.BinaryCrossentropy(from_logits=True),metrics=keras.metrics.BinaryAccuracy())\n",
    "\n",
    "history = model.fit(image_data, labels, epochs=epochs, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.trainable = True\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.SGD(1e-5),\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=tf.keras.metrics.BinaryAccuracy())\n",
    "from tensorflow.keras.callbacks import EarlyStopping, TensorBoard\n",
    "# rm -rf logs\n",
    "%load_ext tensorboard\n",
    "log_folder = 'logs'\n",
    "callbacks = [\n",
    "            EarlyStopping(patience = 5),\n",
    "            TensorBoard(log_dir=log_folder)\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n",
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-25 00:52:07.870114: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4711/4711 [==============================] - ETA: 0s - loss: 0.1450 - binary_accuracy: 0.9491"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-25 12:29:02.996622: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4711/4711 [==============================] - 42253s 9s/step - loss: 0.1450 - binary_accuracy: 0.9491 - val_loss: 0.1260 - val_binary_accuracy: 0.9550\n",
      "Epoch 2/15\n",
      "4711/4711 [==============================] - 4283s 909ms/step - loss: 0.0867 - binary_accuracy: 0.9696 - val_loss: 0.1142 - val_binary_accuracy: 0.9608\n",
      "Epoch 3/15\n",
      "4711/4711 [==============================] - 4158s 883ms/step - loss: 0.0458 - binary_accuracy: 0.9849 - val_loss: 0.1257 - val_binary_accuracy: 0.9656\n",
      "Epoch 4/15\n",
      "4711/4711 [==============================] - 3722s 790ms/step - loss: 0.0229 - binary_accuracy: 0.9929 - val_loss: 0.1395 - val_binary_accuracy: 0.9679\n",
      "Epoch 5/15\n",
      "4711/4711 [==============================] - 3743s 795ms/step - loss: 0.0138 - binary_accuracy: 0.9957 - val_loss: 0.1381 - val_binary_accuracy: 0.9671\n",
      "Epoch 6/15\n",
      "4711/4711 [==============================] - 5619s 1s/step - loss: 0.0108 - binary_accuracy: 0.9965 - val_loss: 0.1377 - val_binary_accuracy: 0.9696\n",
      "Epoch 7/15\n",
      "4711/4711 [==============================] - 4511s 957ms/step - loss: 0.0089 - binary_accuracy: 0.9972 - val_loss: 0.1547 - val_binary_accuracy: 0.9663\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2af83b040>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, TensorBoard\n",
    "# rm -rf logs\n",
    "%load_ext tensorboard\n",
    "log_folder = 'logs'\n",
    "callbacks = [\n",
    "            EarlyStopping(patience = 5),\n",
    "            TensorBoard(log_dir=log_folder)\n",
    "            ]\n",
    "model.fit(image_data, labels, epochs=200,callbacks=callbacks)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8400f417db452b1fc95af2c99e1a3999f9f2bcaa41ebb17e9c014f684ca7b29d"
  },
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
